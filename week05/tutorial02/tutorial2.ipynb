{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"yR8kgpsYBPOW"},"outputs":[],"source":["# # This mounts your Google Drive to the Colab VM.\n","# from google.colab import drive\n","# drive.mount('/content/drive', force_remount=True)\n","\n","# # Enter the foldername in your Drive where you have saved the script and dataset\n","# FOLDERNAME = 'SMM636/'\n","# assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# # Now that we've mounted your Drive, this ensures that\n","# # the Python interpreter of the Colab VM can load\n","# # python files from within it.\n","# import sys\n","# sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"S8snSirvBQfn"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn import neighbors\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import classification_report\n","# load data \n","# titanic = pd.read_csv(\"/content/drive/My Drive/SMM636/train_titanic.csv\")\n","titanic = pd.read_csv(\"train_titanic.csv\")\n","# get feature matrix of training set\n","X = titanic.loc[:, ['Pclass','Parch']] \n","y = titanic.Survived"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":216,"status":"ok","timestamp":1645543981621,"user":{"displayName":"Rui Zhu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01006588934350664975"},"user_tz":0},"id":"SPaAlXJUCbH-"},"outputs":[],"source":["# get training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=105)"]},{"cell_type":"markdown","metadata":{"id":"4rfn16vamJly"},"source":["# **Tune *k* for *k*NN by cross-validation**"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"2L8WBR6el-7w"},"outputs":[{"name":"stdout","output_type":"stream","text":["Best parameters set found on development set:\n","\n","{'n_neighbors': 9}\n","\n","Grid scores on development set:\n","\n","0.596 (+/-0.111) for {'n_neighbors': 1}\n","0.672 (+/-0.058) for {'n_neighbors': 3}\n","0.648 (+/-0.053) for {'n_neighbors': 5}\n","0.666 (+/-0.054) for {'n_neighbors': 7}\n","0.693 (+/-0.055) for {'n_neighbors': 9}\n"]}],"source":["# set tuning values\n","tuned_parameters = [{\"n_neighbors\": [1,3,5,7,9]}]\n","# tune the parameter by k-fold cross-validation\n","# more details of grid search, see 'https://scikit-learn.org/stable/modules/grid_search.html#exhaustive-grid-search'\n","knnCV = GridSearchCV(neighbors.KNeighborsClassifier(), tuned_parameters, scoring='accuracy',cv=5)\n","# scoring can be set to f1, precision, recall, roc_auc and many other metrics, depending on the task and data\n","# more details see 'https://scikit-learn.org/stable/modules/model_evaluation.html'\n","knnCV.fit(X_train, y_train)\n","print(\"Best parameters set found on development set:\")\n","print()\n","print(knnCV.best_params_)\n","print()\n","print(\"Grid scores on development set:\")\n","print()\n","means = knnCV.cv_results_[\"mean_test_score\"]\n","stds = knnCV.cv_results_[\"std_test_score\"]\n","for mean, std, params in zip(means, stds, knnCV.cv_results_[\"params\"]):\n","        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std, params))"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"cNjZ6zThJGeq"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.73      0.76      0.74       165\n","           1       0.59      0.55      0.57       103\n","\n","    accuracy                           0.68       268\n","   macro avg       0.66      0.66      0.66       268\n","weighted avg       0.68      0.68      0.68       268\n","\n"]},{"data":{"text/plain":["0.6791044776119403"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# predict test set labels\n","y_pred = knnCV.predict(X_test)\n","y_true, y_pred = y_test, y_pred\n","print(classification_report(y_true, y_pred))\n","sum(y_pred==y_test)/len(y_test)"]},{"cell_type":"markdown","metadata":{"id":"dy5Ca2zDmds-"},"source":["# **Tune gamma and C for SVM with RBF kernel by cross-validation**"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":236,"status":"ok","timestamp":1645544152926,"user":{"displayName":"Rui Zhu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01006588934350664975"},"user_tz":0},"id":"Rg99l6EhoYzs"},"outputs":[],"source":["from sklearn.svm import SVC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MIffdZaomxz8"},"outputs":[],"source":["tuned_parameters_rbf = [{\"kernel\": [\"rbf\"], \"gamma\": [1,1e-1,1e-2,1e-3, 1e-4], \"C\": [1, 10, 100,1000]}]\n","svmClassifier = GridSearchCV(SVC(), tuned_parameters_rbf, scoring='accuracy',cv=5)\n","svmClassifier.fit(X_train, y_train)\n","print(\"Best parameters set found on development set:\")\n","print()\n","print(svmClassifier.best_params_)\n","print()\n","print(\"Grid scores on development set:\")\n","print()\n","means = svmClassifier.cv_results_[\"mean_test_score\"]\n","stds = svmClassifier.cv_results_[\"std_test_score\"]\n","for mean, std, params in zip(means, stds, svmClassifier.cv_results_[\"params\"]):\n","        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std, params))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t0BHA0BbpRWO"},"outputs":[],"source":["y_pred_svm = svmClassifier.predict(X_test)\n","y_true, y_pred = y_test, y_pred_svm\n","print(classification_report(y_true, y_pred_svm))\n","sum(y_pred_svm==y_test)/len(y_test)"]},{"cell_type":"markdown","metadata":{"id":"ljcd-15arr7h"},"source":["# **Decision tree and random forest**\n","\n","In this part, we are going to know how to fit decision tree and random forest in Python. I am not going to show you the parameter tuning process, to make this part more straightforward. If you want to tune the parameters, you can follow similar steps as in previous sections. \n","\n","**Here we are going to evaluate the performance of the classifier by cross-validation, so cross-validation here is for model evaluation rather than parameter tuning.**"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":541,"status":"ok","timestamp":1645544224901,"user":{"displayName":"Rui Zhu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01006588934350664975"},"user_tz":0},"id":"p8LRLVZZryVV"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import cross_val_score\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EtSXLNfZr4yK"},"outputs":[],"source":["# fit a decision tree and obtain a cross-validation evaluation score\n","dt = DecisionTreeClassifier(ccp_alpha=0.001,random_state=0) \n","# here ccp_alpha is the complexity parameter. you can tune it by cross-validation as in knn and svm. here I use \n","# a fixed value for illustration\n","# evaluate model performance by cross-validation\n","scores_dt = cross_val_score(dt, X, y, cv=5, scoring='accuracy')\n","print(scores_dt)\n","scores_dt.mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HRRs6WInvm1W"},"outputs":[],"source":["# draw a boxplot to visualise the classification performance\n","scores_df=pd.DataFrame(scores_dt,columns=['Accuracy of decision tree'])\n","boxplot=scores_df.boxplot()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xsJGLMVLG1S9"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from sklearn import tree\n","# visualise tree\n","dt_vis=dt.fit(X,y)\n","fn=['Pclass','Parch']\n","cn=['Not Survived','Survived']\n","fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=300)\n","tree.plot_tree(dt_vis,\n","               feature_names = fn, \n","               class_names=cn,\n","               filled = True);\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ol-LLSjmwPPP"},"outputs":[],"source":["# fit a random forest and obtain a cross-validation evaluation score\n","rf = RandomForestClassifier(n_estimators=100,max_features=\"sqrt\",bootstrap=True,oob_score=True,random_state=0) \n","# n_estimator is the number of trees, \n","# max_features is the number of features that are randomly selected to build the tree\n","# bootstrap and oob_score are to get the OOB evaluation\n","rf.fit(X,y)\n","print(rf.oob_score_)\n","# evaluate model performance by cross-validation\n","scores_rf = cross_val_score(rf, X, y, cv=5, scoring='accuracy')\n","print(scores_rf)\n","scores_rf.mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rjGKvpAlyD3J"},"outputs":[],"source":["# compare the classification performance of decision tree and random forest visually\n","scores_df=pd.DataFrame(\n","    {   \"Decision tree\": scores_dt,\n","        \"Random forest\": scores_rf,\n","    })\n","boxplot=scores_df.boxplot()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oWJrBA-oK8Hl"},"outputs":[],"source":["# get variable importance, e.g. mean decrease in gini index\n","rf_var=rf.fit(X,y)\n","importances = rf_var.feature_importances_\n","std = np.std([tree.feature_importances_ for tree in rf_var.estimators_], axis=0)\n","print(importances)\n","print(std)\n","print()\n","\n","forest_importances = pd.Series(importances, index=['Pclass','Parch'])\n","print(forest_importances)\n","\n","fig, ax = plt.subplots()\n","forest_importances.plot.bar(yerr=std, ax=ax)\n","ax.set_title(\"Feature importances using MDI\")\n","ax.set_ylabel(\"Mean decrease in impurity\")\n","fig.tight_layout()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMyzZlSjFnRds4OF0GenwgZ","collapsed_sections":[],"name":"tutorial2.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}
