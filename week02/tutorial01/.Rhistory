# # get training and test set
# train = rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])
# test = rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])
#
# # train = rbind(iris3[sample(iris3[,,1],25)],iris3[sample(iris3[,,2],25)],iris3[sample(iris3[,,3],25)])
# # test = rbind(iris3[sample(iris3[,,1],25)],iris3[sample(iris3[,,2],25)],iris3[sample(iris3[,,3],25)])
# # get class factor
# cl = factor(c(rep("s",25), rep("c",25), rep("v",25)))
# # classification using knn, with k=3
# set.seed(47)
# knn_pred=knn(train, test, cl, k = 3, prob=TRUE)
# # see the attributes of the knn model
# att=attributes(knn_pred)
# # get the probabilities associated with the final prediction
# prob_pred=att$prob
# ######### 1NN ###############
# set.seed(47)
# knn1_pred=knn1(train, test, cl)
# ######### see all probabilities###############
# library(caret)
# set.seed(47)
# knn3_pred=knn3Train(train, test, cl, k = 3, prob=TRUE)
# attributes(knn3_pred)
# ########################################################
# ############### Randomly sample from data###############
# ########################################################
# # get indexes for training data
# n=25 # the number of training data in each class
# NN=dim(iris3)[1]# the total number of observations in each class
# set.seed(983)
# index_s=sample(1:NN,n)
# index_c=sample(1:NN,n)
# index_v=sample(1:NN,n)
# # get training and test set
# train_rand = rbind(iris3[index_s,,1], iris3[index_c,,2], iris3[index_v,,3])
# test_rand = rbind(iris3[-index_s,,1], iris3[-index_c,,2], iris3[-index_v,,3])
# # get class factor for training data
# train_label= factor(c(rep("s",n), rep("c",n), rep("v",n)))
# # get class factor for test data
# test_label_true=factor(c(rep("s",NN-n), rep("c",NN-n), rep("v",NN-n)))
# # classification using knn, with k=5
# kk=5 # number of nearest neighbours
# set.seed(275)
# knn_pred=knn(train=train_rand,test=test_rand,cl=train_label, k=kk, prob=FALSE)
?scale
# Suppose we change Sepal.width to mm while others to m
iris_c=iris[,1:4]
iris_c[,2]=iris[,2]*10
iris_c[,-2]=iris[,c(1,3,4)]/10
summary(iris_c)
# get indexes for training data
n=25 # the number of training data in each class
NN=dim(iris)[1]/3# the total number of observations in each class
set.seed(983)
index_s=sample(which(iris$Species=="setosa"),n)
index_c=sample(which(iris$Species=="versicolor"),n)
index_v=sample(which(iris$Species=="virginica"),n)
# get training and test set
train_rand_c = rbind(iris_c[index_s,], iris_c[index_c,], iris_c[index_v,])
test_rand_c = rbind(iris_c[-c(index_s,index_c,index_v),])
# get class factor for training data
train_label= factor(c(rep("s",n), rep("c",n), rep("v",n)))
# get class factor for test data
test_label_true=factor(c(rep("s",NN-n), rep("c",NN-n), rep("v",NN-n)))
# classification using knn, with k=5
kk=5 # number of nearest neighbours
set.seed(275)
knn_pred=knn(train=train_rand_c,test=test_rand_c,cl=train_label, k=kk, prob=FALSE)
knn_pred
iris_s=scale(iris_c)
# get training and test set
train_rand_s = rbind(iris_s[index_s,], iris_s[index_c,], iris_s[index_v,])
test_rand_s = rbind(iris_s[-c(index_s,index_c,index_v),])
# get class factor for training data
train_label= factor(c(rep("s",n), rep("c",n), rep("v",n)))
# get class factor for test data
test_label_true=factor(c(rep("s",NN-n), rep("c",NN-n), rep("v",NN-n)))
# classification using knn, with k=5
kk=5 # number of nearest neighbours
set.seed(275)
knn_pred=knn(train=train_rand_s,test=test_rand_s,cl=train_label, k=kk, prob=FALSE)
knn_pred
#, echo=FALSE, include=FALSE}
########################################################
############### Scale the data###############
########################################################
library(class)
# get indexes for training data
n=25 # the number of training data in each class
NN=dim(iris)[1]/3# the total number of observations in each class
set.seed(983)
index_s=sample(which(iris$Species=="setosa"),n)
index_c=sample(which(iris$Species=="versicolor"),n)
index_v=sample(which(iris$Species=="virginica"),n)
# Suppose we change Sepal.width to mm while others to m
iris_c=iris[,1:4]
iris_c[,2]=iris[,2]*10
iris_c[,-2]=iris[,-2]/10
# get training and test set
train_rand_c = rbind(iris_c[index_s,], iris_c[index_c,], iris_c[index_v,])
test_rand_c = rbind(iris_c[-c(index_s,index_c,index_v),])
# get class factor for training data
train_label= factor(c(rep("s",n), rep("c",n), rep("v",n)))
# get class factor for test data
test_label_true=factor(c(rep("s",NN-n), rep("c",NN-n), rep("v",NN-n)))
# classification using knn, with k=5
kk=5 # number of nearest neighbours
set.seed(275)
knn_pred=knn(train=train_rand_c,test=test_rand_c,cl=train_label, k=kk, prob=FALSE)
knn_pred
########################################################
#### Now we scale the data first########################
########################################################
iris_s=scale(iris_c)
# get training and test set
train_rand_s = rbind(iris_s[index_s,], iris_s[index_c,], iris_s[index_v,])
test_rand_s = rbind(iris_s[-c(index_s,index_c,index_v),])
# get class factor for training data
train_label= factor(c(rep("s",n), rep("c",n), rep("v",n)))
# get class factor for test data
test_label_true=factor(c(rep("s",NN-n), rep("c",NN-n), rep("v",NN-n)))
# classification using knn, with k=5
kk=5 # number of nearest neighbours
set.seed(275)
knn_pred_s=knn(train=train_rand_s,test=test_rand_s,cl=train_label, k=kk, prob=FALSE)
knn_pred_s
dim(iris)[1]/3
iris_s=scale(iris_c)
structure(iris_s)
iris_s=scale(iris_c)
summary(iris_s)
iris_s=scale(iris_c)
summary(iris_s)
sd(iris_s)
iris_s=scale(iris_c)
summary(iris_s)
sd(iris_s$Sepal.Length)
iris_s=scale(iris_c)
summary(iris_s)
sd(iris_s)
iris_s=scale(iris_c)
summary(iris_s)
sd(iris_s)
?sd
iris_s=scale(iris_c)
summary(iris_s)
sd(iris_s[,1])
iris_s=scale(iris_c)
summary(iris_s)
sd(iris_s[,2])
iris_s=scale(iris_c)
summary(iris_s)
sd(iris_s[,:])
iris_s=scale(iris_c)
summary(iris_s)
sd(iris_s[,1:4])
iris_s=scale(iris_c)
summary(iris_s)
sd(iris_s[,1],iris_s[,2])
iris_s=scale(iris_c)
summary(iris_s)
sd(c(iris_s[,1],iris_s[,2])
iris_s=scale(iris_c)
summary(iris_s)
sd(c(iris_s[,1],iris_s[,2]))
iris_s=scale(iris_c)
summary(iris_s)
sd(c(iris_s[,1])
iris_s=scale(iris_c)
summary(iris_s)
sd(iris_s[,1])
iris_s=scale(iris_c)
summary(iris_s)
sd(iris_s[,1])
apply(iris_s, 2, sd)
iris_s=scale(iris_c)
summary(iris_s)
sd(iris_s[,1])
apply(iris_s, 1, sd)
iris_s=scale(iris_c)
summary(iris_s)
sd(iris_s[,1])
apply(iris_s, 3, sd)
iris_s=scale(iris_c)
summary(iris_s)
sd(iris_s[,1])
apply(iris_s, 2, sd)
iris_s=scale(iris_c)
summary(iris_s)
apply(iris_s, 2, sd)
apply(iris_s, 2, mean)
iris_s=scale(iris_c)
summary(iris_s)
apply(iris_s, 2, mean)
apply(iris_s, 2, sd)
#, echo=FALSE, include=FALSE}
########################################################
############### Scale the data###############
########################################################
library(class)
# get indexes for training data
n=25 # the number of training data in each class
NN=dim(iris)[1]/3# the total number of observations in each class
set.seed(983)
index_s=sample(which(iris$Species=="setosa"),n)
index_c=sample(which(iris$Species=="versicolor"),n)
index_v=sample(which(iris$Species=="virginica"),n)
# Suppose we change Sepal.width to mm while others to m
iris_c=iris[,1:4]
iris_c[,2]=iris[,2]*10
iris_c[,-2]=iris[,-2]/10
# get training and test set
train_rand_c = rbind(iris_c[index_s,], iris_c[index_c,], iris_c[index_v,])
test_rand_c = rbind(iris_c[-c(index_s,index_c,index_v),])
# get class factor for training data
train_label= factor(c(rep("s",n), rep("c",n), rep("v",n)))
# get class factor for test data
test_label_true=factor(c(rep("s",NN-n), rep("c",NN-n), rep("v",NN-n)))
# classification using knn, with k=5
kk=5 # number of nearest neighbours
set.seed(275)
knn_pred=knn(train=train_rand_c,test=test_rand_c,cl=train_label, k=kk, prob=FALSE)
knn_pred
########################################################
#### Now we scale the data first########################
########################################################
iris_s=scale(iris_c)
# get training and test set
train_rand_s = rbind(iris_s[index_s,], iris_s[index_c,], iris_s[index_v,])
test_rand_s = rbind(iris_s[-c(index_s,index_c,index_v),])
# get class factor for training data
train_label= factor(c(rep("s",n), rep("c",n), rep("v",n)))
# get class factor for test data
test_label_true=factor(c(rep("s",NN-n), rep("c",NN-n), rep("v",NN-n)))
# classification using knn, with k=5
kk=5 # number of nearest neighbours
set.seed(275)
knn_pred_s=knn(train=train_rand_s,test=test_rand_s,cl=train_label, k=kk, prob=FALSE)
knn_pred_s
knn_pred_s == test_label_true
#, echo=FALSE, include=FALSE}
########################################################
############### Scale the data###############
########################################################
library(class)
# get indexes for training data
n=25 # the number of training data in each class
NN=dim(iris)[1]/3# the total number of observations in each class
set.seed(983)
index_s=sample(which(iris$Species=="setosa"),n)
index_c=sample(which(iris$Species=="versicolor"),n)
index_v=sample(which(iris$Species=="virginica"),n)
# Suppose we change Sepal.width to mm while others to m
iris_c=iris[,1:4]
iris_c[,2]=iris[,2]*10
iris_c[,-2]=iris[,-2]/10
# get training and test set
train_rand_c = rbind(iris_c[index_s,], iris_c[index_c,], iris_c[index_v,])
test_rand_c = rbind(iris_c[-c(index_s,index_c,index_v),])
# get class factor for training data
train_label= factor(c(rep("s",n), rep("c",n), rep("v",n)))
# get class factor for test data
test_label_true=factor(c(rep("s",NN-n), rep("c",NN-n), rep("v",NN-n)))
# classification using knn, with k=5
kk=5 # number of nearest neighbours
set.seed(275)
knn_pred=knn(train=train_rand_c,test=test_rand_c,cl=train_label, k=kk, prob=FALSE)
knn_pred
########################################################
#### Now we scale the data first########################
########################################################
iris_s=scale(iris_c)
# get training and test set
train_rand_s = rbind(iris_s[index_s,], iris_s[index_c,], iris_s[index_v,])
test_rand_s = rbind(iris_s[-c(index_s,index_c,index_v),])
# get class factor for training data
train_label= factor(c(rep("s",n), rep("c",n), rep("v",n)))
# get class factor for test data
test_label_true=factor(c(rep("s",NN-n), rep("c",NN-n), rep("v",NN-n)))
# classification using knn, with k=5
kk=5 # number of nearest neighbours
set.seed(275)
knn_pred_s=knn(train=train_rand_s,test=test_rand_s,cl=train_label, k=kk, prob=TRUE)
knn_pred_s
knn_pred_s == test_label_true
#, echo=FALSE, include=FALSE}
########################################################
############### Scale the data###############
########################################################
library(class)
# get indexes for training data
n=25 # the number of training data in each class
NN=dim(iris)[1]/3# the total number of observations in each class
set.seed(983)
index_s=sample(which(iris$Species=="setosa"),n)
index_c=sample(which(iris$Species=="versicolor"),n)
index_v=sample(which(iris$Species=="virginica"),n)
# Suppose we change Sepal.width to mm while others to m
iris_c=iris[,1:4]
iris_c[,2]=iris[,2]*10
iris_c[,-2]=iris[,-2]/10
# get training and test set
train_rand_c = rbind(iris_c[index_s,], iris_c[index_c,], iris_c[index_v,])
test_rand_c = rbind(iris_c[-c(index_s,index_c,index_v),])
# get class factor for training data
train_label= factor(c(rep("s",n), rep("c",n), rep("v",n)))
# get class factor for test data
test_label_true=factor(c(rep("s",NN-n), rep("c",NN-n), rep("v",NN-n)))
# classification using knn, with k=5
kk=5 # number of nearest neighbours
set.seed(275)
knn_pred=knn(train=train_rand_c,test=test_rand_c,cl=train_label, k=kk, prob=FALSE)
knn_pred
########################################################
#### Now we scale the data first########################
########################################################
iris_s=scale(iris_c)
# get training and test set
train_rand_s = rbind(iris_s[index_s,], iris_s[index_c,], iris_s[index_v,])
test_rand_s = rbind(iris_s[-c(index_s,index_c,index_v),])
# get class factor for training data
train_label= factor(c(rep("s",n), rep("c",n), rep("v",n)))
# get class factor for test data
test_label_true=factor(c(rep("s",NN-n), rep("c",NN-n), rep("v",NN-n)))
# classification using knn, with k=5
kk=5 # number of nearest neighbours
set.seed(275)
knn_pred_s=knn(train=train_rand_s,test=test_rand_s,cl=train_label, k=kk, prob=FALSE)
knn_pred_s
# get training and test set
train_rand_s = rbind(iris_s[index_s,], iris_s[index_c,], iris_s[index_v,])
test_rand_s = rbind(iris_s[-c(index_s,index_c,index_v),])
# get class factor for training data
train_label= factor(c(rep("s",n), rep("c",n), rep("v",n)))
# get class factor for test data
test_label_true=factor(c(rep("s",NN-n), rep("c",NN-n), rep("v",NN-n)))
# classification using knn, with k=5
kk=5 # number of nearest neighbours
set.seed(275)
knn_pred=knn(train=train_rand_s,test=test_rand_s,cl=train_label, k=kk, prob=TRUE)
knn_pred
knn_pred_s == test_label_true
knn_pred
# get training and test set
train_rand_s = rbind(iris_s[index_s,], iris_s[index_c,], iris_s[index_v,])
test_rand_s = rbind(iris_s[-c(index_s,index_c,index_v),])
# get class factor for training data
train_label= factor(c(rep("s",n), rep("c",n), rep("v",n)))
# get class factor for test data
test_label_true=factor(c(rep("s",NN-n), rep("c",NN-n), rep("v",NN-n)))
# classification using knn, with k=5
kk=5 # number of nearest neighbours
set.seed(275)
knn_pred=knn(train=train_rand_s,test=test_rand_s,cl=train_label, k=kk, prob=TRUE)
knn_pred_s == test_label_true
# get training and test set
train_rand_s = rbind(iris_s[index_s,], iris_s[index_c,], iris_s[index_v,])
test_rand_s = rbind(iris_s[-c(index_s,index_c,index_v),])
# get class factor for training data
train_label= factor(c(rep("s",n), rep("c",n), rep("v",n)))
# get class factor for test data
test_label_true=factor(c(rep("s",NN-n), rep("c",NN-n), rep("v",NN-n)))
# classification using knn, with k=5
kk=5 # number of nearest neighbours
set.seed(275)
knn_pred=knn(train=train_rand_s,test=test_rand_s,cl=train_label, k=kk, prob=TRUE)
knn_pred
knn_pred_s == test_label_true
setwd("/Volumes/GoogleDrive-117044175360160401988/My Drive/github/machine-learning-smm636/week02/tutorial01")
library(shiny)
library(shiny)
# Define UI for app that draws a histogram ----
ui = fluidPage(
# App title ----
titlePanel("Hello Shiny!"),
# Sidebar layout with input and output definitions ----
sidebarLayout(
# Sidebar panel for inputs ----
sidebarPanel(
# Input: Slider for the number of bins ----
sliderInput(inputId = "bins",
label = "Number of bins:",
min = 1,
max = 50,
value = 30)
),
# Main panel for displaying outputs ----
mainPanel(
# Output: Histogram ----
plotOutput(outputId = "distPlot")
)
)
)
# Define server logic required to draw a histogram ----
server = function(input, output) {
# Histogram of the Old Faithful Geyser Data ----
# with requested number of bins
# This expression that generates a histogram is wrapped in a call
# to renderPlot to indicate that:
#
# 1. It is "reactive" and therefore should be automatically
#    re-executed when inputs (input$bins) change
# 2. Its output type is a plot
output$distPlot = renderPlot({
x    = faithful$waiting
bins = seq(min(x), max(x), length.out = input$bins + 1)
hist(x, breaks = bins, col = "#75AADB", border = "orange",
xlab = "Waiting time to next eruption (in mins)",
main = "Histogram of waiting times")
})
}
shinyApp(ui = ui, server = server)
library(shiny)
# Define UI for app that draws a histogram ----
ui = fluidPage(
# App title ----
titlePanel("Hello Shiny!"),
# Sidebar layout with input and output definitions ----
sidebarLayout(
# Sidebar panel for inputs ----
sidebarPanel(
# Input: Slider for the number of bins ----
sliderInput(inputId = "bins",
label = "Number of bins:",
min = 1,
max = 50,
value = 30)
),
# Main panel for displaying outputs ----
mainPanel(
# Output: Histogram ----
plotOutput(outputId = "distPlot")
)
)
)
# Define server logic required to draw a histogram ----
server = function(input, output) {
# Histogram of the Old Faithful Geyser Data ----
# with requested number of bins
# This expression that generates a histogram is wrapped in a call
# to renderPlot to indicate that:
#
# 1. It is "reactive" and therefore should be automatically
#    re-executed when inputs (input$bins) change
# 2. Its output type is a plot
output$distPlot = renderPlot({
x    = faithful$waiting
bins = seq(min(x), max(x), length.out = input$bins + 1)
hist(x, breaks = bins, col = "#75AADB", border = "orange",
xlab = "Waiting time to next eruption (in mins)",
main = "Histogram of waiting times")
})
}
shinyApp(ui = ui, server = server)
x    = faithful$waiting
bins = seq(min(x), max(x), length.out = 20 + 1)
hist(x, breaks = bins, col = "#75AADB", border = "white",
xlab = "Waiting time to next eruption (in mins)",
main = "Histogram of waiting times")
library(shiny)
# Define UI for app that draws a histogram ----
ui = fluidPage(
# App title ----
titlePanel("Find k nearest neighbours"),
# Sidebar layout with input and output definitions ----
sidebarLayout(
# Sidebar panel for inputs ----
sidebarPanel(
# Input: Slider for the number of bins ----
sliderInput(inputId = "k",
label = "Number of nearest neighbours:",
min = 1,
max = 11,
value = 1,
step=2)
),
# Main panel for displaying outputs ----
mainPanel(
# Output: scatter plot ----
plotOutput(outputId = "scatterPlot")
)
)
)
# Define server logic required to draw a histogram ----
server = function(input, output) {
output$scatterPlot = renderPlot({
##### load training data
set.seed(10)
library(MASS)
class1=mvrnorm(n = 30, mu=c(1,1), Sigma=matrix(c(1,0,0,1),2,2))
class2=mvrnorm(n = 30, mu=c(2,2), Sigma=matrix(c(1,0,0,1),2,2))
train.feature=rbind(class1,class2)
train.label=c(rep(1,30),rep(2,30))
##### scatter plot of the training data
plot(train.feature,col=ifelse(train.label==1, "blue", "red"),
pch=ifelse(train.label==1, 16, 17),xlab="x1",ylab="x2",cex=1.5)
points(2.1,1.5,pch=15,col="green",cex=1.5) ## test point
##### find k nearest neighbours
library("FNN")
test.feature=t(c(2.1,1.5))
k=input$k
knearest=get.knnx(train.feature,test.feature, k)
neighbour=train.feature[knearest$nn.index,]
if(k==1){
points(t(neighbour),pch=0,col="black",cex=1.8)}else{
points((neighbour),pch=0,col="black",cex=1.8)
}
})
}
shinyApp(ui = ui, server = server)
c(1,1)
t(c(1,1))
