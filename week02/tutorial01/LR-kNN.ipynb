{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LR-kNN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPNoF6IrZkcYSKFCnPl18rY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Please ignore this part if you don't use Colab.\n","# This mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Enter the foldername in your Drive where you have saved the script and dataset\n","FOLDERNAME = 'SMM636/'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# Now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))"],"metadata":{"id":"MqB7SNgHkWIF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Titanic - Machine Learning from Disaster**\n","\n","* This task is to predict survival on the Titanic, which is a challenge provided by Kaggle: 'https://www.kaggle.com/c/titanic/overview'. The training set and test set are already split for you to use.\n","\n","* There are ten variables in this dataset: \n","\n","  **One Label**: *Survival*: 0=No, 1=Yes\n","\n","  **Nine Features**:\n","  *pclass* (ticket class); *sex*; *age*; *sibsp* (# of siblings / spouses aboard); *parch* (# of parents / children aboard); *ticket* (ticket number); *fare* (passenger fare); *cabin* (cabin number); *embarked* (port of embarkation, C=Cherbourg, Q=Queenstown, S=Southampton)\n","\n","* This exercise is to get familiar with using Python for classification, so we just use two features in the dataset for illustration purpose."],"metadata":{"id":"dLZ-GOZ3qL-p"}},{"cell_type":"markdown","source":["# **Logistic regression via `sklearn`**"],"metadata":{"id":"-ZmialSgBC2I"}},{"cell_type":"code","source":["import pandas as pd\n","# load training data \n","train = pd.read_csv(\"/content/drive/My Drive/SMM636/train_titanic.csv\")\n","# have a look at the training data\n","train.head()\n","#train.info()"],"metadata":{"id":"LyjylLDJn5DK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get feature matrix of training set\n","features_train = train.loc[:, ['Pclass','Parch']] \n","# DataFrame.loc: Access a group of rows and columns by label(s) or a boolean array.\n","# 'https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html'\n","print(features_train.head())\n","features_train.shape"],"metadata":{"id":"bpXpAHuivOUY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get labels of training set\n","labels_train = train.Survived\n","labels_train.shape"],"metadata":{"id":"G-TbBGmQwmae"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# build a logistic regression model\n","from sklearn.linear_model import LogisticRegression\n","# initialise a logistic regression model\n","# lr = LogisticRegression()\n","lr = LogisticRegression(penalty='none') # by default a penalty term is added \n","# train the model by the training feature matrix and labels\n","lr.fit(features_train,labels_train)\n","# or combine the previous two steps in one line, the results are the same\n","lr = LogisticRegression(penalty='none').fit(features_train,labels_train)"],"metadata":{"id":"mY8tIjlGx6uS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# have a look at the estimated coefficients and intercept\n","print(lr.coef_, lr.intercept_)"],"metadata":{"id":"bpjcQtxs0k9x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import the test set\n","test = pd.read_csv(\"/content/drive/My Drive/SMM636/test_titanic.csv\")\n","# get the feature matrix of test set\n","features_test = test.loc[:, ['Pclass','Parch']] \n","# note that there are no labels in test set, so we can only have our predictions, but cannot know how\n","# the classifier performs"],"metadata":{"id":"Yi9Bh4Cl07Ph"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get prediction of survival from logistic regression\n","pred = lr.predict(features_test)\n","pred[0:9]"],"metadata":{"id":"wQDye0q02HsJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Logistic regression via `statsmodels`**"],"metadata":{"id":"22gVGcJgBXhy"}},{"cell_type":"code","source":["# however, as a statistical model, we usually want to have an easy access to the estimated coefficients,\n","# their p-values and other statistical quantities, as what we can easily have in R\n","# in this case, I would recommend to use the statsmodels library\n","# 'https://www.statsmodels.org/dev/examples/notebooks/generated/glm.html'\n","import statsmodels.api as sm\n","# we need to manually add a constant column to include intercept in regression\n","features_train_new=sm.add_constant(features_train, prepend=False) \n","print(features_train_new.head())\n","# fit a GLM model with binomial family\n","lrs=sm.GLM(labels_train,features_train_new,family=sm.families.Binomial()).fit()\n","print(lrs.summary())"],"metadata":{"id":"V50UEGp84qxT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# predict for test set\n","features_test_new = sm.add_constant(features_test, prepend=False) \n","scores_new = lrs.predict(features_test_new)\n","scores_new[0:9] # here we have the scores (posterior probability) rather than labels from prediction"],"metadata":{"id":"5ERlqNAr97Qs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# transform scores to labels: <0.5 --> 0; >0.5 --> 1\n","predict_new = scores_new # initialise the predicted label vector\n","predict_new.loc[predict_new<0.5] = 0\n","predict_new.loc[predict_new>=0.5] = 1\n","predict_new=predict_new.astype(int)\n","print(predict_new[0:9])"],"metadata":{"id":"fDKhRxBJ_CfU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ***k*NN via `sklearn`**\n","\n","In this part, we are goint to know how to get training/test splits by `train_test_split` function. Thus we are going to use the training set only."],"metadata":{"id":"lBzqawYzsDj7"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn import neighbors\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"pttpiCkitDvY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We now name the matrices X and y to avoid confusion. We are going to split the dataset to a training and test set.\n","X=features_train \n","y=labels_train\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=105)\n","X_train.head()"],"metadata":{"id":"GxYVXjaSuptt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(y_train[0:9])\n","# how to check how many training instances for each class?\n","print(sum(y_train==1))\n","np.count_nonzero(y_train==0)"],"metadata":{"id":"vJYOHT07yo7_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# build a knn classifier based on the training set\n","n_neighbours=5\n","KNNClassifier = neighbors.KNeighborsClassifier(n_neighbours, weights=\"uniform\")\n","KNNClassifier.fit(X_train, y_train)"],"metadata":{"id":"lxUDfqggzWYW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# predict the test set\n","y_pred=KNNClassifier.predict(X_test)\n","# get the predicted probabilities for each class\n","print(KNNClassifier.predict_proba(X_test)[0:9,])\n","# have a look at the prediction\n","print(y_pred[0:9])\n","# get the accuracy\n","print(sum(y_pred==y_test)/len(y_test))\n","print(accuracy_score(y_pred, y_test))"],"metadata":{"id":"rrCOUbaSzbyZ"},"execution_count":null,"outputs":[]}]}